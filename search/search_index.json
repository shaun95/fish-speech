{"config":{"lang":["zh","en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4ecb\u7ecd","text":"<p>Warning</p> <p>\u6211\u4eec\u4e0d\u5bf9\u4ee3\u7801\u5e93\u7684\u4efb\u4f55\u975e\u6cd5\u4f7f\u7528\u627f\u62c5\u4efb\u4f55\u8d23\u4efb. \u8bf7\u53c2\u9605\u60a8\u5f53\u5730\u5173\u4e8e DMCA (\u6570\u5b57\u5343\u5e74\u6cd5\u6848) \u548c\u5176\u4ed6\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4.</p> <p>\u6b64\u4ee3\u7801\u5e93\u6839\u636e <code>BSD-3-Clause</code> \u8bb8\u53ef\u8bc1\u53d1\u5e03, \u6240\u6709\u6a21\u578b\u6839\u636e CC-BY-NC-SA-4.0 \u8bb8\u53ef\u8bc1\u53d1\u5e03.</p> <p> </p>"},{"location":"#_2","title":"\u8981\u6c42","text":"<ul> <li>GPU \u5185\u5b58: 4GB (\u7528\u4e8e\u63a8\u7406), 16GB (\u7528\u4e8e\u5fae\u8c03)</li> <li>\u7cfb\u7edf: Linux, Windows</li> </ul>"},{"location":"#windows","title":"Windows \u914d\u7f6e","text":"<p>Windows \u4e13\u4e1a\u7528\u6237\u53ef\u4ee5\u8003\u8651 WSL2 \u6216 docker \u6765\u8fd0\u884c\u4ee3\u7801\u5e93\u3002</p> <p>Windows \u975e\u4e13\u4e1a\u7528\u6237\u53ef\u8003\u8651\u4ee5\u4e0b\u4e3a\u514d Linux \u73af\u5883\u7684\u57fa\u7840\u8fd0\u884c\u65b9\u6cd5\uff08\u9644\u5e26\u6a21\u578b\u7f16\u8bd1\u529f\u80fd\uff0c\u5373 <code>torch.compile</code>\uff09\uff1a</p> <ol> <li>\u89e3\u538b\u9879\u76ee\u538b\u7f29\u5305\u3002</li> <li>\u70b9\u51fb install_env.bat \u5b89\u88c5\u73af\u5883\u3002       <ul> <li>\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 install_env.bat \u7684 <code>USE_MIRROR</code> \u9879\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u955c\u50cf\u7ad9\u4e0b\u8f7d\u3002</li> <li><code>USE_MIRROR=false</code> \u4f7f\u7528\u539f\u59cb\u7ad9\u4e0b\u8f7d\u6700\u65b0\u7a33\u5b9a\u7248 <code>torch</code> \u73af\u5883\u3002<code>USE_MIRROR=true</code> \u4e3a\u4ece\u955c\u50cf\u7ad9\u4e0b\u8f7d\u6700\u65b0 <code>torch</code> \u73af\u5883\u3002\u9ed8\u8ba4\u4e3a <code>true</code>\u3002</li> <li>\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 install_env.bat \u7684 <code>INSTALL_TYPE</code> \u9879\u6765\u51b3\u5b9a\u662f\u5426\u542f\u7528\u53ef\u7f16\u8bd1\u73af\u5883\u4e0b\u8f7d\u3002</li> <li><code>INSTALL_TYPE=preview</code> \u4e0b\u8f7d\u5f00\u53d1\u7248\u7f16\u8bd1\u73af\u5883\u3002<code>INSTALL_TYPE=stable</code> \u4e0b\u8f7d\u7a33\u5b9a\u7248\u4e0d\u5e26\u7f16\u8bd1\u73af\u5883\u3002</li> </ul> </li> <li>\u82e5\u7b2c2\u6b65 INSTALL_TYPE=preview \u5219\u6267\u884c\u8fd9\u4e00\u6b65\uff08\u53ef\u8df3\u8fc7\uff0c\u6b64\u6b65\u4e3a\u6fc0\u6d3b\u7f16\u8bd1\u6a21\u578b\u73af\u5883\uff09       <ol> <li>\u4f7f\u7528\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7d LLVM \u7f16\u8bd1\u5668\u3002                <ul> <li>LLVM-17.0.6\uff08\u539f\u7ad9\u7ad9\u70b9\u4e0b\u8f7d\uff09</li> <li>LLVM-17.0.6\uff08\u955c\u50cf\u7ad9\u70b9\u4e0b\u8f7d\uff09</li> <li>\u4e0b\u8f7d\u5b8c LLVM-17.0.6-win64.exe \u540e\uff0c\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5\uff0c\u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u4f4d\u7f6e\uff0c\u6700\u91cd\u8981\u7684\u662f\u52fe\u9009 <code>Add Path to Current User</code> \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u3002</li> <li>\u786e\u8ba4\u5b89\u88c5\u5b8c\u6210\u3002</li> </ul> </li> <li>\u4e0b\u8f7d\u5b89\u88c5 Microsoft Visual C++ \u53ef\u518d\u53d1\u884c\u7a0b\u5e8f\u5305\uff0c\u89e3\u51b3\u6f5c\u5728 .dll \u4e22\u5931\u95ee\u9898\u3002                <ul> <li>MSVC++ 14.40.33810.0 \u4e0b\u8f7d</li> </ul> </li> <li>\u4e0b\u8f7d\u5b89\u88c5 Visual Studio \u793e\u533a\u7248\u4ee5\u83b7\u53d6 MSVC++ \u7f16\u8bd1\u5de5\u5177, \u89e3\u51b3 LLVM \u7684\u5934\u6587\u4ef6\u4f9d\u8d56\u95ee\u9898\u3002                <ul> <li>Visual Studio \u4e0b\u8f7d</li> <li>\u5b89\u88c5\u597dVisual Studio Installer\u4e4b\u540e\uff0c\u4e0b\u8f7dVisual Studio Community 2022</li> <li>\u5982\u4e0b\u56fe\u70b9\u51fb<code>\u4fee\u6539</code>\u6309\u94ae\uff0c\u627e\u5230<code>\u4f7f\u7528C++\u7684\u684c\u9762\u5f00\u53d1</code>\u9879\uff0c\u52fe\u9009\u4e0b\u8f7d</li> <p> </p> </ul> </li> </ol> </li> <li>\u53cc\u51fb start.bat, \u8fdb\u5165 Fish-Speech \u8bad\u7ec3\u63a8\u7406\u914d\u7f6e WebUI \u9875\u9762\u3002       <ul> <li>(\u53ef\u9009) \u60f3\u76f4\u63a5\u8fdb\u5165\u63a8\u7406\u9875\u9762\uff1f\u7f16\u8f91\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u7684 <code>API_FLAGS.txt</code>, \u524d\u4e09\u884c\u4fee\u6539\u6210\u5982\u4e0b\u683c\u5f0f:                <pre><code>--infer\n# --api\n# --listen ...\n...</code></pre> </li> <li>(\u53ef\u9009) \u60f3\u542f\u52a8 API \u670d\u52a1\u5668\uff1f\u7f16\u8f91\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u7684 <code>API_FLAGS.txt</code>, \u524d\u4e09\u884c\u4fee\u6539\u6210\u5982\u4e0b\u683c\u5f0f:                <pre><code># --infer\n--api\n--listen ...\n...</code></pre> </li> </ul> </li> <li>\uff08\u53ef\u9009\uff09\u53cc\u51fb <code>run_cmd.bat</code> \u8fdb\u5165\u672c\u9879\u76ee\u7684 conda/python \u547d\u4ee4\u884c\u73af\u5883</li> </ol>"},{"location":"#linux","title":"Linux \u914d\u7f6e","text":"<pre><code># \u521b\u5efa\u4e00\u4e2a python 3.10 \u865a\u62df\u73af\u5883, \u4f60\u4e5f\u53ef\u4ee5\u7528 virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# \u5b89\u88c5 pytorch\npip3 install torch torchvision torchaudio\n\n# \u5b89\u88c5 fish-speech\npip3 install -e .\n\n# (Ubuntu / Debian \u7528\u6237) \u5b89\u88c5 sox\napt install libsox-dev\n</code></pre>"},{"location":"#_3","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li>2024/05/10: \u66f4\u65b0\u4e86 Fish-Speech \u5230 1.1 \u7248\u672c\uff0c\u5f15\u5165\u4e86 VITS Decoder \u6765\u964d\u4f4e\u53e3\u80e1\u548c\u63d0\u9ad8\u97f3\u8272\u76f8\u4f3c\u5ea6.</li> <li>2024/04/22: \u5b8c\u6210\u4e86 Fish-Speech 1.0 \u7248\u672c, \u5927\u5e45\u4fee\u6539\u4e86 VQGAN \u548c LLAMA \u6a21\u578b.</li> <li>2023/12/28: \u6dfb\u52a0\u4e86 <code>lora</code> \u5fae\u8c03\u652f\u6301.</li> <li>2023/12/27: \u6dfb\u52a0\u4e86 <code>gradient checkpointing</code>, <code>causual sampling</code> \u548c <code>flash-attn</code> \u652f\u6301.</li> <li>2023/12/19: \u66f4\u65b0\u4e86 Webui \u548c HTTP API.</li> <li>2023/12/18: \u66f4\u65b0\u4e86\u5fae\u8c03\u6587\u6863\u548c\u76f8\u5173\u4f8b\u5b50.</li> <li>2023/12/17: \u66f4\u65b0\u4e86 <code>text2semantic</code> \u6a21\u578b, \u652f\u6301\u65e0\u97f3\u7d20\u6a21\u5f0f.</li> <li>2023/12/13: \u6d4b\u8bd5\u7248\u53d1\u5e03, \u5305\u542b VQGAN \u6a21\u578b\u548c\u4e00\u4e2a\u57fa\u4e8e LLAMA \u7684\u8bed\u8a00\u6a21\u578b (\u53ea\u652f\u6301\u97f3\u7d20).</li> </ul>"},{"location":"#_4","title":"\u81f4\u8c22","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> <li>GPT-SoVITS</li> </ul>"},{"location":"finetune/","title":"\u5fae\u8c03","text":"<p>\u663e\u7136, \u5f53\u4f60\u6253\u5f00\u8fd9\u4e2a\u9875\u9762\u7684\u65f6\u5019, \u4f60\u5df2\u7ecf\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b few-shot \u7684\u6548\u679c\u4e0d\u7b97\u6ee1\u610f. \u4f60\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b, \u4f7f\u5f97\u5b83\u5728\u4f60\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d.  </p> <p><code>Fish Speech</code> \u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210: <code>VQGAN</code>,<code>LLAMA</code>, \u4ee5\u53ca <code>VITS Decoder</code>. </p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u5148\u8fdb\u884c\u5982\u4e0b\u6d4b\u8bd5\u6765\u5224\u65ad\u4f60\u662f\u5426\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code> <pre><code>python tools/vqgan/inference.py -i test.wav\npython tools/vits_decoder/inference.py \\\n    -ckpt checkpoints/vits_decoder_v1.1.ckpt \\\n    -i fake.npy -r test.wav \\\n    --text \"\u5408\u6210\u6587\u672c\"\n</code></pre> \u8be5\u6d4b\u8bd5\u4f1a\u751f\u6210\u4e00\u4e2a <code>fake.wav</code> \u6587\u4ef6, \u5982\u679c\u8be5\u6587\u4ef6\u7684\u97f3\u8272\u548c\u8bf4\u8bdd\u4eba\u7684\u97f3\u8272\u4e0d\u540c, \u6216\u8005\u8d28\u91cf\u4e0d\u9ad8, \u4f60\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code>.</p> <p>\u76f8\u5e94\u7684, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u6765\u8fd0\u884c <code>generate.py</code>, \u5224\u65ad\u97f5\u5f8b\u662f\u5426\u6ee1\u610f, \u5982\u679c\u4e0d\u6ee1\u610f, \u5219\u9700\u8981\u5fae\u8c03 <code>LLAMA</code>.</p> <p>\u5efa\u8bae\u5148\u5bf9 <code>LLAMA</code> \u8fdb\u884c\u5fae\u8c03\uff0c\u6700\u540e\u518d\u6839\u636e\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code>.</p>"},{"location":"finetune/#llama","title":"LLAMA \u5fae\u8c03","text":""},{"location":"finetune/#1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u5efa\u8bae\u4e3a <code>.lab</code>.</p> <p>Warning</p> <p>\u5efa\u8bae\u5148\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u54cd\u5ea6\u5339\u914d, \u4f60\u53ef\u4ee5\u4f7f\u7528 fish-audio-preprocess \u6765\u5b8c\u6210\u8fd9\u4e00\u6b65\u9aa4.  <pre><code>fap loudness-norm data-raw data --clean\n</code></pre></p>"},{"location":"finetune/#2-token","title":"2. \u6279\u91cf\u63d0\u53d6\u8bed\u4e49 token","text":"<p>\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 vqgan \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\n</code></pre> <p>\u968f\u540e\u53ef\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u63d0\u53d6\u8bed\u4e49 token:</p> <pre><code>python tools/vqgan/extract_vq.py data \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u8c03\u6574 <code>--num-workers</code> \u548c <code>--batch-size</code> \u6765\u63d0\u9ad8\u63d0\u53d6\u901f\u5ea6, \u4f46\u662f\u8bf7\u6ce8\u610f\u4e0d\u8981\u8d85\u8fc7\u4f60\u7684\u663e\u5b58\u9650\u5236.  </p> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>.npy</code> \u6587\u4ef6, \u5982\u4e0b\u6240\u793a:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"finetune/#3-protobuf","title":"3. \u6253\u5305\u6570\u636e\u96c6\u4e3a protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --input \"data\" \\\n    --output \"data/quantized-dataset-ft.protos\" \\\n    --text-extension .lab \\\n    --num-workers 16\n</code></pre> <p>\u547d\u4ee4\u6267\u884c\u5b8c\u6bd5\u540e, \u4f60\u5e94\u8be5\u80fd\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u770b\u5230 <code>quantized-dataset-ft.protos</code> \u6587\u4ef6.</p>"},{"location":"finetune/#4","title":"4. \u6700\u540e, \u542f\u52a8\u5fae\u8c03","text":"<p>\u540c\u6837\u7684, \u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 <code>LLAMA</code> \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\n</code></pre> <p>\u6700\u540e, \u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u5fae\u8c03:</p> <pre><code>python fish_speech/train.py --config-name text2semantic_finetune \\\n    model@model.model=dual_ar_2_codebook_medium\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/text2semantic_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570\u5982 <code>batch_size</code>, <code>gradient_accumulation_steps</code> \u7b49, \u6765\u9002\u5e94\u4f60\u7684\u663e\u5b58.</p> <p>Note</p> <p>\u5bf9\u4e8e Windows \u7528\u6237, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>trainer.strategy.process_group_backend=gloo</code> \u6765\u907f\u514d <code>nccl</code> \u7684\u95ee\u9898.</p> <p>\u8bad\u7ec3\u7ed3\u675f\u540e, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u90e8\u5206, \u5e76\u643a\u5e26 <code>--speaker SPK1</code> \u53c2\u6570\u6765\u6d4b\u8bd5\u4f60\u7684\u6a21\u578b.</p> <p>Info</p> <p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b, \u57fa\u672c\u53ea\u4f1a\u5b66\u5230\u8bf4\u8bdd\u4eba\u7684\u53d1\u97f3\u65b9\u5f0f, \u800c\u4e0d\u5305\u542b\u97f3\u8272, \u4f60\u4f9d\u7136\u9700\u8981\u4f7f\u7528 prompt \u6765\u4fdd\u8bc1\u97f3\u8272\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u60f3\u8981\u5b66\u5230\u97f3\u8272, \u8bf7\u5c06\u8bad\u7ec3\u6b65\u6570\u8c03\u5927, \u4f46\u8fd9\u6709\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408.</p>"},{"location":"finetune/#lora","title":"\u4f7f\u7528 LoRA \u8fdb\u884c\u5fae\u8c03","text":"<p>Note</p> <p>LoRA \u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u8fc7\u62df\u5408\u7684\u98ce\u9669, \u4f46\u662f\u76f8\u5e94\u7684\u4f1a\u5bfc\u81f4\u5728\u5927\u6570\u636e\u96c6\u4e0a\u6b20\u62df\u5408.   </p> <p>\u5982\u679c\u4f60\u60f3\u4f7f\u7528 LoRA, \u8bf7\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u6570 <code>+lora@model.lora_config=r_8_alpha_16</code>.  </p> <p>\u8bad\u7ec3\u5b8c\u6210\u540e, \u4f60\u9700\u8981\u5148\u5c06 loRA \u7684\u6743\u91cd\u8f6c\u4e3a\u666e\u901a\u6743\u91cd, \u7136\u540e\u518d\u8fdb\u884c\u63a8\u7406.</p> <pre><code>python tools/llama/merge_lora.py \\\n    --llama-config dual_ar_2_codebook_medium \\\n    --lora-config r_8_alpha_16 \\\n    --llama-weight checkpoints/text2semantic-sft-medium-v1.1-4k.pth \\\n    --lora-weight results/text2semantic-finetune-medium-lora/checkpoints/step_000000200.ckpt \\\n    --output checkpoints/merged.ckpt\n</code></pre>"},{"location":"finetune/#vits","title":"VITS \u5fae\u8c03","text":""},{"location":"finetune/#1_1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>Note</p> <p>VITS \u5fae\u8c03\u76ee\u524d\u4ec5\u652f\u6301 <code>.lab</code> \u4f5c\u4e3a\u6807\u7b7e\u6587\u4ef6\uff0c\u4e0d\u652f\u6301 <code>filelist</code> \u5f62\u5f0f.</p> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u5efa\u8bae\u4e3a <code>.lab</code>.</p>"},{"location":"finetune/#2","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/vq_train_filelist.txt</code> \u548c <code>data/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vits_decoder_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vits_decoder_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u5982\u6570\u636e\u96c6\u914d\u7f6e.</p>"},{"location":"finetune/#4_1","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path results/vits_decoder_finetune/checkpoints/step_000010000.ckpt \\\n    -i test.npy -r test.wav \\\n    --text \"\u5408\u6210\u6587\u672c\"\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p>"},{"location":"finetune/#vqgan","title":"VQGAN \u5fae\u8c03 (\u4e0d\u63a8\u8350)","text":"<p>\u5728 V1.1 \u7248\u672c\u4e2d, \u6211\u4eec\u4e0d\u518d\u63a8\u8350\u4f7f\u7528 VQGAN \u8fdb\u884c\u5fae\u8c03, \u4f7f\u7528 VITS Decoder \u4f1a\u83b7\u5f97\u66f4\u597d\u7684\u8868\u73b0, \u4f46\u662f\u5982\u679c\u4f60\u4ecd\u7136\u60f3\u8981\u4f7f\u7528 VQGAN \u8fdb\u884c\u5fae\u8c03, \u4f60\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u6b65\u9aa4.</p>"},{"location":"finetune/#1_2","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>.</p>"},{"location":"finetune/#2_1","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/vq_train_filelist.txt</code> \u548c <code>data/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3_1","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vqgan_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u4f46\u5927\u90e8\u5206\u60c5\u51b5\u4e0b, \u4f60\u4e0d\u9700\u8981\u8fd9\u4e48\u505a.</p>"},{"location":"finetune/#4_2","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p> <p>Note</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u5176\u4ed6\u7684 checkpoint, \u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528\u6700\u65e9\u7684\u6ee1\u8db3\u4f60\u8981\u6c42\u7684 checkpoint, \u4ed6\u4eec\u901a\u5e38\u5728 OOD \u4e0a\u8868\u73b0\u66f4\u597d.</p>"},{"location":"inference/","title":"\u63a8\u7406","text":"<p>\u63a8\u7406\u652f\u6301\u547d\u4ee4\u884c, http api, \u4ee5\u53ca webui \u4e09\u79cd\u65b9\u5f0f.  </p> <p>Note</p> <p>\u603b\u7684\u6765\u8bf4, \u63a8\u7406\u5206\u4e3a\u51e0\u4e2a\u90e8\u5206:  </p> <ol> <li>\u7ed9\u5b9a\u4e00\u6bb5 ~10 \u79d2\u7684\u8bed\u97f3, \u5c06\u5b83\u7528 VQGAN \u7f16\u7801.  </li> <li>\u5c06\u7f16\u7801\u540e\u7684\u8bed\u4e49 token \u548c\u5bf9\u5e94\u6587\u672c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4f8b\u5b50.  </li> <li>\u7ed9\u5b9a\u4e00\u6bb5\u65b0\u6587\u672c, \u8ba9\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u7684\u8bed\u4e49 token.  </li> <li>\u5c06\u751f\u6210\u7684\u8bed\u4e49 token \u8f93\u5165 VITS / VQGAN \u89e3\u7801, \u751f\u6210\u5bf9\u5e94\u7684\u8bed\u97f3.  </li> </ol> <p>\u5728 V1.1 \u7248\u672c\u4e2d, \u6211\u4eec\u63a8\u8350\u4f18\u5148\u4f7f\u7528 VITS \u89e3\u7801\u5668, \u56e0\u4e3a\u5b83\u5728\u97f3\u8d28\u548c\u53e3\u80e1\u4e0a\u90fd\u6709\u66f4\u597d\u7684\u8868\u73b0.</p>"},{"location":"inference/#_2","title":"\u547d\u4ee4\u884c\u63a8\u7406","text":"<p>\u4ece\u6211\u4eec\u7684 huggingface \u4ed3\u5e93\u4e0b\u8f7d\u6240\u9700\u7684 <code>vqgan</code> \u548c <code>text2semantic</code> \u6a21\u578b\u3002</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 vits_decoder_v1.1.ckpt --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 firefly-gan-base-generator.ckpt --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4f7f\u7528mirror\u4e0b\u8f7d\u3002</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vits_decoder_v1.1.ckpt --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 firefly-gan-base-generator.ckpt --local-dir checkpoints\n</code></pre>"},{"location":"inference/#1-prompt","title":"1. \u4ece\u8bed\u97f3\u751f\u6210 prompt:","text":"<p>Note</p> <p>\u5982\u679c\u4f60\u6253\u7b97\u8ba9\u6a21\u578b\u968f\u673a\u9009\u62e9\u97f3\u8272, \u4f60\u53ef\u4ee5\u8df3\u8fc7\u8fd9\u4e00\u6b65.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> \u4f60\u5e94\u8be5\u80fd\u5f97\u5230\u4e00\u4e2a <code>fake.npy</code> \u6587\u4ef6.</p>"},{"location":"inference/#2-token","title":"2. \u4ece\u6587\u672c\u751f\u6210\u8bed\u4e49 token:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"\u8981\u8f6c\u6362\u7684\u6587\u672c\" \\\n    --prompt-text \"\u4f60\u7684\u53c2\u8003\u6587\u672c\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --config-name dual_ar_2_codebook_medium \\\n    --checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u521b\u5efa <code>codes_N</code> \u6587\u4ef6, \u5176\u4e2d N \u662f\u4ece 0 \u5f00\u59cb\u7684\u6574\u6570.</p> <p>Note</p> <p>\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528 <code>--compile</code> \u6765\u878d\u5408 cuda \u5185\u6838\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406 (~30 \u4e2a token/\u79d2 -&gt; ~500 \u4e2a token/\u79d2). \u5bf9\u5e94\u7684, \u5982\u679c\u4f60\u4e0d\u6253\u7b97\u4f7f\u7528\u52a0\u901f, \u4f60\u53ef\u4ee5\u6ce8\u91ca\u6389 <code>--compile</code> \u53c2\u6570.</p> <p>Info</p> <p>\u5bf9\u4e8e\u4e0d\u652f\u6301 bf16 \u7684 GPU, \u4f60\u53ef\u80fd\u9700\u8981\u4f7f\u7528 <code>--half</code> \u53c2\u6570.</p> <p>Warning</p> <p>\u5982\u679c\u4f60\u5728\u4f7f\u7528\u81ea\u5df1\u5fae\u8c03\u7684\u6a21\u578b, \u8bf7\u52a1\u5fc5\u643a\u5e26 <code>--speaker</code> \u53c2\u6570\u6765\u4fdd\u8bc1\u53d1\u97f3\u7684\u7a33\u5b9a\u6027.</p>"},{"location":"inference/#3-token","title":"3. \u4ece\u8bed\u4e49 token \u751f\u6210\u4eba\u58f0:","text":""},{"location":"inference/#vits","title":"VITS \u89e3\u7801","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path checkpoints/vits_decoder_v1.1.ckpt \\\n    -i codes_0.npy -r ref.wav \\\n    --text \"\u8981\u751f\u6210\u7684\u6587\u672c\"\n</code></pre>"},{"location":"inference/#vqgan","title":"VQGAN \u89e3\u7801 (\u4e0d\u63a8\u8350)","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre>"},{"location":"inference/#http-api","title":"HTTP API \u63a8\u7406","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:</p> <pre><code>python -m tools.api \\\n    --listen 0.0.0.0:8000 \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --decoder-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --decoder-config-name vqgan_pretrain\n\n# \u63a8\u8350\u4e2d\u56fd\u5927\u9646\u7528\u6237\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:\nHF_ENDPOINT=https://hf-mirror.com python -m ...\n</code></pre> <p>\u968f\u540e, \u4f60\u53ef\u4ee5\u5728 <code>http://127.0.0.1:8000/</code> \u4e2d\u67e5\u770b\u5e76\u6d4b\u8bd5 API.</p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u4f7f\u7528\u4ee5\u4e0b\u53c2\u6570\u6765\u542f\u52a8 VITS \u89e3\u7801\u5668:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # \u6216\u8005\u4f60\u81ea\u5df1\u7684\u6a21\u578b\n</code></pre>"},{"location":"inference/#webui","title":"WebUI \u63a8\u7406","text":"<p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 WebUI:</p> <pre><code>python -m tools.webui \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --decoder-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --decoder-config-name vqgan_pretrain\n</code></pre> <p>Info</p> <p>\u4f60\u5e94\u8be5\u4f7f\u7528\u4ee5\u4e0b\u53c2\u6570\u6765\u542f\u52a8 VITS \u89e3\u7801\u5668:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # \u6216\u8005\u4f60\u81ea\u5df1\u7684\u6a21\u578b\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528 Gradio \u73af\u5883\u53d8\u91cf, \u5982 <code>GRADIO_SHARE</code>, <code>GRADIO_SERVER_PORT</code>, <code>GRADIO_SERVER_NAME</code> \u6765\u914d\u7f6e WebUI.</p> <p>\u795d\u5927\u5bb6\u73a9\u5f97\u5f00\u5fc3!</p>"},{"location":"samples/","title":"\u4f8b\u5b50","text":""},{"location":"samples/#1","title":"\u4e2d\u6587\u53e5\u5b50 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u949f\u79bb (\u539f\u795e) \u8299\u5b81\u5a1c (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2","title":"\u4e2d\u6587\u53e5\u5b50 2","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#3","title":"\u4e2d\u6587\u53e5\u5b50 3","text":"<pre><code>\u5927\u5bb6\u597d\uff0c\u6211\u662f Fish Audio \u5f00\u53d1\u7684\u5f00\u6e90\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u7ecf\u8fc7\u5341\u4e94\u4e07\u5c0f\u65f6\u7684\u6570\u636e\u8bad\u7ec3\uff0c\n\u6211\u5df2\u7ecf\u80fd\u591f\u719f\u7ec3\u638c\u63e1\u4e2d\u6587\u3001\u65e5\u8bed\u548c\u82f1\u8bed\uff0c\u6211\u7684\u8bed\u8a00\u5904\u7406\u80fd\u529b\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u58f0\u97f3\u8868\u73b0\u5f62\u5f0f\u4e30\u5bcc\u591a\u53d8\u3002\n\u4f5c\u4e3a\u4e00\u4e2a\u4ec5\u6709\u4ebf\u7ea7\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6211\u76f8\u4fe1\u793e\u533a\u6210\u5458\u80fd\u591f\u5728\u4e2a\u4eba\u8bbe\u5907\u4e0a\u8f7b\u677e\u8fd0\u884c\u548c\u5fae\u8c03\uff0c\u8ba9\u6211\u6210\u4e3a\u60a8\u7684\u79c1\u4eba\u8bed\u97f3\u52a9\u624b\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#1_1","title":"\u82f1\u6587\u53e5\u5b50 1","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2_1","title":"\u82f1\u6587\u53e5\u5b50 2","text":"<pre><code>Hello everyone, I am an open-source text-to-speech model developed by \nFish Audio. After training with 150,000 hours of data, I have become proficient \nin Chinese, Japanese, and English, and my language processing abilities \nare close to human level. My voice is capable of a wide range of expressions. \nAs a model with only hundreds of millions of parameters, I believe community \nmembers can easily run and fine-tune me on their personal devices, allowing \nme to serve as your personal voice assistant.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#1_2","title":"\u65e5\u6587\u53e5\u5b50 1","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2_2","title":"\u65e5\u6587\u53e5\u5b50 2","text":"<pre><code>\u7686\u3055\u3093\u3001\u3053\u3093\u306b\u3061\u306f\u3002\u79c1\u306f\u30d5\u30a3\u30c3\u30b7\u30e5\u30aa\u30fc\u30c7\u30a3\u30aa\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30c6\n\u30ad\u30b9\u30c8\u304b\u3089\u97f3\u58f0\u3078\u306e\u5909\u63db\u30e2\u30c7\u30eb\u3067\u3059\u300215\u4e07\u6642\u9593\u306e\u30c7\u30fc\u30bf\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u7d4c\u3066\u3001\n\u4e2d\u56fd\u8a9e\u3001\u65e5\u672c\u8a9e\u3001\u82f1\u8a9e\u3092\u719f\u77e5\u3057\u3066\u304a\u308a\u3001\u8a00\u8a9e\u51e6\u7406\u80fd\u529b\u306f\u4eba\u9593\u306b\u8fd1\u3044\u30ec\u30d9\u30eb\u3067\u3059\u3002\n\u58f0\u306e\u8868\u73fe\u3082\u591a\u5f69\u3067\u8c4a\u304b\u3067\u3059\u3002\u6570\u5104\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6301\u3064\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\n\u306e\u30e1\u30f3\u30d0\u30fc\u304c\u500b\u4eba\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u7c21\u5358\u306b\u5b9f\u884c\u3057\u3001\u5fae\u8abf\u6574\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\n\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u79c1\u3092\u500b\u4eba\u306e\u97f3\u58f0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3068\u3057\u3066\u6d3b\u7528\u3067\u304d\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"en/","title":"Introduction","text":"<p>Warning</p> <p>We assume no responsibility for any illegal use of the codebase. Please refer to the local laws regarding DMCA (Digital Millennium Copyright Act) and other relevant laws in your area.</p> <p>This codebase is released under the <code>BSD-3-Clause</code> license, and all models are released under the CC-BY-NC-SA-4.0 license.</p> <p> </p>"},{"location":"en/#requirements","title":"Requirements","text":"<ul> <li>GPU Memory: 4GB (for inference), 16GB (for fine-tuning)</li> <li>System: Linux, Windows</li> </ul>"},{"location":"en/#windows-setup","title":"Windows Setup","text":"<p>Windows professional users may consider WSL2 or Docker to run the codebase.</p> <p>Non-professional Windows users can consider the following methods to run the codebase without a Linux environment (with model compilation capabilities aka <code>torch.compile</code>):</p> <ol> <li>Unzip the project package.</li> <li>Click <code>install_env.bat</code> to install the environment.       <ul> <li>You can decide whether to use a mirror site for downloads by editing the <code>USE_MIRROR</code> item in <code>install_env.bat</code>.</li> <li><code>USE_MIRROR=false</code> downloads the latest stable version of <code>torch</code> from the original site. <code>USE_MIRROR=true</code> downloads the latest version of <code>torch</code> from a mirror site. The default is <code>true</code>.</li> <li>You can decide whether to enable the compiled environment download by editing the <code>INSTALL_TYPE</code> item in <code>install_env.bat</code>.</li> <li><code>INSTALL_TYPE=preview</code> downloads the preview version with the compiled environment. <code>INSTALL_TYPE=stable</code> downloads the stable version without the compiled environment.</li> </ul> </li> <li>If step 2 has <code>USE_MIRROR=preview</code>, execute this step (optional, for activating the compiled model environment):       <ol> <li>Download the LLVM compiler using the following links:                <ul> <li>LLVM-17.0.6 (original site download)</li> <li>LLVM-17.0.6 (mirror site download)</li> <li>After downloading <code>LLVM-17.0.6-win64.exe</code>, double-click to install it, choose an appropriate installation location, and most importantly, check <code>Add Path to Current User</code> to add to the environment variables.</li> <li>Confirm the installation is complete.</li> </ul> </li> <li>Download and install the Microsoft Visual C++ Redistributable package to resolve potential .dll missing issues.                <ul> <li>MSVC++ 14.40.33810.0 Download</li> </ul> </li> <li>Download and install Visual Studio Community Edition to obtain MSVC++ build tools, resolving LLVM header file dependencies.                <ul> <li>Visual Studio Download</li> <li>After installing Visual Studio Installer, download Visual Studio Community 2022.</li> <li>Click the <code>Modify</code> button as shown below, find the <code>Desktop development with C++</code> option, and check it for download.</li> <p> </p> </ul> </li> </ol> </li> <li>Double-click <code>start.bat</code> to enter the Fish-Speech training inference configuration WebUI page.       <ul> <li>(Optional) Want to go directly to the inference page? Edit the <code>API_FLAGS.txt</code> in the project root directory and modify the first three lines as follows:                <pre><code>--infer\n# --api\n# --listen ...\n...</code></pre> </li> <li>(Optional) Want to start the API server? Edit the <code>API_FLAGS.txt</code> in the project root directory and modify the first three lines as follows:                <pre><code># --infer\n--api\n--listen ...\n...</code></pre> </li> </ul> </li> <li>(Optional) Double-click <code>run_cmd.bat</code> to enter the conda/python command line environment of this project.</li> </ol>"},{"location":"en/#linux-setup","title":"Linux Setup","text":"<pre><code># Create a python 3.10 virtual environment, you can also use virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# Install pytorch\npip3 install torch torchvision torchaudio\n\n# Install fish-speech\npip3 install -e .\n\n# (Ubuntu / Debian User) Install sox\napt install libsox-dev\n</code></pre>"},{"location":"en/#changelog","title":"Changelog","text":"<ul> <li>2024/05/10: Updated Fish-Speech to 1.1 version, implement VITS decoder to reduce WER and improve timbre similarity.</li> <li>2024/04/22: Finished Fish-Speech 1.0 version, significantly modified VQGAN and LLAMA models.</li> <li>2023/12/28: Added <code>lora</code> fine-tuning support.</li> <li>2023/12/27: Add <code>gradient checkpointing</code>, <code>causual sampling</code>, and <code>flash-attn</code> support.</li> <li>2023/12/19: Updated webui and HTTP API.</li> <li>2023/12/18: Updated fine-tuning documentation and related examples.</li> <li>2023/12/17: Updated <code>text2semantic</code> model, supporting phoneme-free mode.</li> <li>2023/12/13: Beta version released, includes VQGAN model and a language model based on LLAMA (phoneme support only).</li> </ul>"},{"location":"en/#acknowledgements","title":"Acknowledgements","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> <li>GPT-SoVITS</li> </ul>"},{"location":"en/finetune/","title":"Fine-tuning","text":"<p>Obviously, when you opened this page, you were not satisfied with the performance of the few-shot pre-trained model. You want to fine-tune a model to improve its performance on your dataset.</p> <p><code>Fish Speech</code> consists of three modules: <code>VQGAN</code>, <code>LLAMA</code>, and <code>VITS</code>.</p> <p>Info</p> <p>You should first conduct the following test to determine if you need to fine-tune <code>VITS Decoder</code>: <pre><code>python tools/vqgan/inference.py -i test.wav\npython tools/vits_decoder/inference.py \\\n    -ckpt checkpoints/vits_decoder_v1.1.ckpt \\\n    -i fake.npy -r test.wav \\\n    --text \"The text you want to generate\"\n</code></pre> This test will generate a <code>fake.wav</code> file. If the timbre of this file differs from the speaker's original voice, or if the quality is not high, you need to fine-tune <code>VITS Decoder</code>.</p> <p>Similarly, you can refer to Inference to run <code>generate.py</code> and evaluate if the prosody meets your expectations. If it does not, then you need to fine-tune <code>LLAMA</code>.</p> <p>It is recommended to fine-tune the LLAMA first, then fine-tune the <code>VITS Decoder</code> according to your needs.</p>"},{"location":"en/finetune/#fine-tuning-llama","title":"Fine-tuning LLAMA","text":""},{"location":"en/finetune/#1-prepare-the-dataset","title":"1. Prepare the dataset","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>You need to convert your dataset into the above format and place it under <code>data</code>. The audio file can have the extensions <code>.mp3</code>, <code>.wav</code>, or <code>.flac</code>, and the annotation file should have the extensions <code>.lab</code>.</p> <p>Warning</p> <p>It's recommended to apply loudness normalization to the dataset. You can use fish-audio-preprocess to do this.</p> <pre><code>fap loudness-norm data-raw data --clean\n</code></pre>"},{"location":"en/finetune/#2-batch-extraction-of-semantic-tokens","title":"2. Batch extraction of semantic tokens","text":"<p>Make sure you have downloaded the VQGAN weights. If not, run the following command:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\n</code></pre> <p>You can then run the following command to extract semantic tokens:</p> <pre><code>python tools/vqgan/extract_vq.py data \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> <p>Note</p> <p>You can adjust <code>--num-workers</code> and <code>--batch-size</code> to increase extraction speed, but please make sure not to exceed your GPU memory limit. For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>.</p> <p>This command will create <code>.npy</code> files in the <code>data</code> directory, as shown below:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"en/finetune/#3-pack-the-dataset-into-protobuf","title":"3. Pack the dataset into protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --input \"data\" \\\n    --output \"data/quantized-dataset-ft.protos\" \\\n    --text-extension .lab \\\n    --num-workers 16\n</code></pre> <p>After the command finishes executing, you should see the <code>quantized-dataset-ft.protos</code> file in the <code>data</code> directory.</p>"},{"location":"en/finetune/#4-finally-start-the-fine-tuning","title":"4. Finally, start the fine-tuning","text":"<p>Similarly, make sure you have downloaded the <code>LLAMA</code> weights. If not, run the following command:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\n</code></pre> <p>Finally, you can start the fine-tuning by running the following command: <pre><code>python fish_speech/train.py --config-name text2semantic_finetune \\\n    model@model.model=dual_ar_2_codebook_medium\n</code></pre></p> <p>Note</p> <p>You can modify the training parameters such as <code>batch_size</code>, <code>gradient_accumulation_steps</code>, etc. to fit your GPU memory by modifying <code>fish_speech/configs/text2semantic_finetune.yaml</code>.</p> <p>Note</p> <p>For Windows users, you can use <code>trainer.strategy.process_group_backend=gloo</code> to avoid <code>nccl</code> issues.</p> <p>After training is complete, you can refer to the inference section, and use <code>--speaker SPK1</code> to generate speech.</p> <p>Info</p> <p>By default, the model will only learn the speaker's speech patterns and not the timbre. You still need to use prompts to ensure timbre stability. If you want to learn the timbre, you can increase the number of training steps, but this may lead to overfitting.</p>"},{"location":"en/finetune/#fine-tuning-with-lora","title":"Fine-tuning with LoRA","text":"<p>Note</p> <p>LoRA can reduce the risk of overfitting in models, but it may also lead to underfitting on large datasets. </p> <p>If you want to use LoRA, please add the following parameter: <code>+lora@model.lora_config=r_8_alpha_16</code>. </p> <p>After training, you need to convert the LoRA weights to regular weights before performing inference.</p> <pre><code>python tools/llama/merge_lora.py \\\n    --llama-config dual_ar_2_codebook_medium \\\n    --lora-config r_8_alpha_16 \\\n    --llama-weight checkpoints/text2semantic-sft-medium-v1.1-4k.pth \\\n    --lora-weight results/text2semantic-finetune-medium-lora/checkpoints/step_000000200.ckpt \\\n    --output checkpoints/merged.ckpt\n</code></pre>"},{"location":"en/finetune/#fine-tuning-vits-decoder","title":"Fine-tuning VITS Decoder","text":""},{"location":"en/finetune/#1-prepare-the-dataset_1","title":"1. Prepare the Dataset","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>Note</p> <p>VITS fine-tuning currently only supports <code>.lab</code> as the label file and does not support the <code>filelist</code> format.</p> <p>You need to format your dataset as shown above and place it under <code>data</code>. Audio files can have <code>.mp3</code>, <code>.wav</code>, or <code>.flac</code> extensions, and the annotation files should have the <code>.lab</code> extension.</p>"},{"location":"en/finetune/#2-split-training-and-validation-sets","title":"2. Split Training and Validation Sets","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>This command will create <code>data/vq_train_filelist.txt</code> and <code>data/vq_val_filelist.txt</code> in the <code>data/demo</code> directory, to be used for training and validation respectively.</p> <p>Info</p> <p>For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>. Please note that the audio files in <code>filelist</code> must also be located in the <code>data</code> folder.</p>"},{"location":"en/finetune/#3-start-training","title":"3. Start Training","text":"<pre><code>python fish_speech/train.py --config-name vits_decoder_finetune\n</code></pre> <p>Note</p> <p>You can modify training parameters by editing <code>fish_speech/configs/vits_decoder_finetune.yaml</code>, but in most cases, this won't be necessary.</p>"},{"location":"en/finetune/#4-test-the-audio","title":"4. Test the Audio","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path results/vits_decoder_finetune/checkpoints/step_000010000.ckpt \\\n    -i test.npy -r test.wav \\\n    --text \"The text you want to generate\"\n</code></pre> <p>You can review <code>fake.wav</code> to assess the fine-tuning results.</p>"},{"location":"en/finetune/#fine-tuning-vqgan-not-recommended","title":"Fine-tuning VQGAN (Not Recommended)","text":"<p>We no longer recommend using VQGAN for fine-tuning in version 1.1. Using VITS Decoder will yield better results, but if you still want to fine-tune VQGAN, you can refer to the following steps.</p>"},{"location":"en/finetune/#1-prepare-the-dataset_2","title":"1. Prepare the Dataset","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>You need to format your dataset as shown above and place it under <code>data</code>. Audio files can have <code>.mp3</code>, <code>.wav</code>, or <code>.flac</code> extensions.</p>"},{"location":"en/finetune/#2-split-training-and-validation-sets_1","title":"2. Split Training and Validation Sets","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>This command will create <code>data/vq_train_filelist.txt</code> and <code>data/vq_val_filelist.txt</code> in the <code>data/demo</code> directory, to be used for training and validation respectively.</p> <p>Info</p> <p>For the VITS format, you can specify a file list using <code>--filelist xxx.list</code>. Please note that the audio files in <code>filelist</code> must also be located in the <code>data</code> folder.</p>"},{"location":"en/finetune/#3-start-training_1","title":"3. Start Training","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>You can modify training parameters by editing <code>fish_speech/configs/vqgan_finetune.yaml</code>, but in most cases, this won't be necessary.</p>"},{"location":"en/finetune/#4-test-the-audio_1","title":"4. Test the Audio","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>You can review <code>fake.wav</code> to assess the fine-tuning results.</p> <p>Note</p> <p>You may also try other checkpoints. We suggest using the earliest checkpoint that meets your requirements, as they often perform better on out-of-distribution (OOD) data.</p>"},{"location":"en/inference/","title":"Inference","text":"<p>Inference support command line, HTTP API and web UI.</p> <p>Note</p> <p>Overall, reasoning consists of several parts:</p> <ol> <li>Encode a given ~10 seconds of voice using VQGAN.</li> <li>Input the encoded semantic tokens and the corresponding text into the language model as an example.</li> <li>Given a new piece of text, let the model generate the corresponding semantic tokens.</li> <li>Input the generated semantic tokens into VITS / VQGAN to decode and generate the corresponding voice.</li> </ol> <p>In version 1.1, we recommend using VITS for decoding, as it performs better than VQGAN in both timbre and pronunciation.</p>"},{"location":"en/inference/#command-line-inference","title":"Command Line Inference","text":"<p>Download the required <code>vqgan</code> and <code>text2semantic</code> models from our Hugging Face repository.</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 vits_decoder_v1.1.ckpt --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 firefly-gan-base-generator.ckpt --local-dir checkpoints\n</code></pre>"},{"location":"en/inference/#1-generate-prompt-from-voice","title":"1. Generate prompt from voice:","text":"<p>Note</p> <p>If you plan to let the model randomly choose a voice timbre, you can skip this step.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> You should get a <code>fake.npy</code> file.</p>"},{"location":"en/inference/#2-generate-semantic-tokens-from-text","title":"2. Generate semantic tokens from text:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"The text you want to convert\" \\\n    --prompt-text \"Your reference text\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --config-name dual_ar_2_codebook_medium \\\n    --checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>This command will create a <code>codes_N</code> file in the working directory, where N is an integer starting from 0.</p> <p>Note</p> <p>You may want to use <code>--compile</code> to fuse CUDA kernels for faster inference (~30 tokens/second -&gt; ~500 tokens/second). Correspondingly, if you do not plan to use acceleration, you can comment out the <code>--compile</code> parameter.</p> <p>Info</p> <p>For GPUs that do not support bf16, you may need to use the <code>--half</code> parameter.</p> <p>Warning</p> <p>If you are using your own fine-tuned model, please be sure to carry the <code>--speaker</code> parameter to ensure the stability of pronunciation.</p>"},{"location":"en/inference/#3-generate-vocals-from-semantic-tokens","title":"3. Generate vocals from semantic tokens:","text":""},{"location":"en/inference/#vits-decoder","title":"VITS Decoder","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path checkpoints/vits_decoder_v1.1.ckpt \\\n    -i codes_0.npy -r ref.wav \\\n    --text \"The text you want to generate\"\n</code></pre>"},{"location":"en/inference/#vqgan-decoder-not-recommended","title":"VQGAN Decoder (not recommended)","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre>"},{"location":"en/inference/#http-api-inference","title":"HTTP API Inference","text":"<p>We provide a HTTP API for inference. You can use the following command to start the server:</p> <pre><code>python -m tools.api \\\n    --listen 0.0.0.0:8000 \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --decoder-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --decoder-config-name vqgan_pretrain\n</code></pre> <p>After that, you can view and test the API at http://127.0.0.1:8000/.  </p> <p>Info</p> <p>You should use following parameters to start VITS decoder:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # or your own model\n</code></pre>"},{"location":"en/inference/#webui-inference","title":"WebUI Inference","text":"<p>You can start the WebUI using the following command:</p> <pre><code>python -m tools.webui \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --vqgan-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --vits-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\"\n</code></pre> <p>Info</p> <p>You should use following parameters to start VITS decoder:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # or your own model\n</code></pre> <p>Note</p> <p>You can use Gradio environment variables, such as <code>GRADIO_SHARE</code>, <code>GRADIO_SERVER_PORT</code>, <code>GRADIO_SERVER_NAME</code> to configure WebUI.</p> <p>Enjoy!</p>"},{"location":"en/samples/","title":"Samples","text":""},{"location":"en/samples/#chinese-sentence-1","title":"Chinese Sentence 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Nahida (Genshin Impact) Zhongli (Genshin Impact) Furina (Genshin Impact) Random Speaker 1  -  Random Speaker 2  -"},{"location":"en/samples/#chinese-sentence-2","title":"Chinese Sentence 2","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> Speaker Input Audio Synthesized Audio Nahida (Genshin Impact) Random Speaker  -"},{"location":"en/samples/#chinese-sentence-3","title":"Chinese Sentence 3","text":"<pre><code>\u5927\u5bb6\u597d\uff0c\u6211\u662f Fish Audio \u5f00\u53d1\u7684\u5f00\u6e90\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u7ecf\u8fc7\u5341\u4e94\u4e07\u5c0f\u65f6\u7684\u6570\u636e\u8bad\u7ec3\uff0c\n\u6211\u5df2\u7ecf\u80fd\u591f\u719f\u7ec3\u638c\u63e1\u4e2d\u6587\u3001\u65e5\u8bed\u548c\u82f1\u8bed\uff0c\u6211\u7684\u8bed\u8a00\u5904\u7406\u80fd\u529b\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u58f0\u97f3\u8868\u73b0\u5f62\u5f0f\u4e30\u5bcc\u591a\u53d8\u3002\n\u4f5c\u4e3a\u4e00\u4e2a\u4ec5\u6709\u4ebf\u7ea7\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6211\u76f8\u4fe1\u793e\u533a\u6210\u5458\u80fd\u591f\u5728\u4e2a\u4eba\u8bbe\u5907\u4e0a\u8f7b\u677e\u8fd0\u884c\u548c\u5fae\u8c03\uff0c\u8ba9\u6211\u6210\u4e3a\u60a8\u7684\u79c1\u4eba\u8bed\u97f3\u52a9\u624b\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker  -"},{"location":"en/samples/#english-sentence-1","title":"English Sentence 1","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker 1  -  Random Speaker 2  -"},{"location":"en/samples/#english-sentence-2","title":"English Sentence 2","text":"<pre><code>Hello everyone, I am an open-source text-to-speech model developed by \nFish Audio. After training with 150,000 hours of data, I have become proficient \nin Chinese, Japanese, and English, and my language processing abilities \nare close to human level. My voice is capable of a wide range of expressions. \nAs a model with only hundreds of millions of parameters, I believe community \nmembers can easily run and fine-tune me on their personal devices, allowing \nme to serve as your personal voice assistant.\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker  -"},{"location":"en/samples/#japanese-sentence-1","title":"Japanese Sentence 1","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker 1  -  Random Speaker 2  -"},{"location":"en/samples/#japanese-sentence-2","title":"Japanese Sentence 2","text":"<pre><code>\u7686\u3055\u3093\u3001\u3053\u3093\u306b\u3061\u306f\u3002\u79c1\u306f\u30d5\u30a3\u30c3\u30b7\u30e5\u30aa\u30fc\u30c7\u30a3\u30aa\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30c6\n\u30ad\u30b9\u30c8\u304b\u3089\u97f3\u58f0\u3078\u306e\u5909\u63db\u30e2\u30c7\u30eb\u3067\u3059\u300215\u4e07\u6642\u9593\u306e\u30c7\u30fc\u30bf\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u7d4c\u3066\u3001\n\u4e2d\u56fd\u8a9e\u3001\u65e5\u672c\u8a9e\u3001\u82f1\u8a9e\u3092\u719f\u77e5\u3057\u3066\u304a\u308a\u3001\u8a00\u8a9e\u51e6\u7406\u80fd\u529b\u306f\u4eba\u9593\u306b\u8fd1\u3044\u30ec\u30d9\u30eb\u3067\u3059\u3002\n\u58f0\u306e\u8868\u73fe\u3082\u591a\u5f69\u3067\u8c4a\u304b\u3067\u3059\u3002\u6570\u5104\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6301\u3064\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\n\u306e\u30e1\u30f3\u30d0\u30fc\u304c\u500b\u4eba\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u7c21\u5358\u306b\u5b9f\u884c\u3057\u3001\u5fae\u8abf\u6574\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\n\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u79c1\u3092\u500b\u4eba\u306e\u97f3\u58f0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3068\u3057\u3066\u6d3b\u7528\u3067\u304d\u307e\u3059\u3002\n</code></pre> Speaker Input Audio Synthesized Audio Random Speaker  -"},{"location":"","title":"\u4ecb\u7ecd","text":"<p>Warning</p> <p>\u6211\u4eec\u4e0d\u5bf9\u4ee3\u7801\u5e93\u7684\u4efb\u4f55\u975e\u6cd5\u4f7f\u7528\u627f\u62c5\u4efb\u4f55\u8d23\u4efb. \u8bf7\u53c2\u9605\u60a8\u5f53\u5730\u5173\u4e8e DMCA (\u6570\u5b57\u5343\u5e74\u6cd5\u6848) \u548c\u5176\u4ed6\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4.</p> <p>\u6b64\u4ee3\u7801\u5e93\u6839\u636e <code>BSD-3-Clause</code> \u8bb8\u53ef\u8bc1\u53d1\u5e03, \u6240\u6709\u6a21\u578b\u6839\u636e CC-BY-NC-SA-4.0 \u8bb8\u53ef\u8bc1\u53d1\u5e03.</p> <p> </p>"},{"location":"#_2","title":"\u8981\u6c42","text":"<ul> <li>GPU \u5185\u5b58: 4GB (\u7528\u4e8e\u63a8\u7406), 16GB (\u7528\u4e8e\u5fae\u8c03)</li> <li>\u7cfb\u7edf: Linux, Windows</li> </ul>"},{"location":"#windows","title":"Windows \u914d\u7f6e","text":"<p>Windows \u4e13\u4e1a\u7528\u6237\u53ef\u4ee5\u8003\u8651 WSL2 \u6216 docker \u6765\u8fd0\u884c\u4ee3\u7801\u5e93\u3002</p> <p>Windows \u975e\u4e13\u4e1a\u7528\u6237\u53ef\u8003\u8651\u4ee5\u4e0b\u4e3a\u514d Linux \u73af\u5883\u7684\u57fa\u7840\u8fd0\u884c\u65b9\u6cd5\uff08\u9644\u5e26\u6a21\u578b\u7f16\u8bd1\u529f\u80fd\uff0c\u5373 <code>torch.compile</code>\uff09\uff1a</p> <ol> <li>\u89e3\u538b\u9879\u76ee\u538b\u7f29\u5305\u3002</li> <li>\u70b9\u51fb install_env.bat \u5b89\u88c5\u73af\u5883\u3002       <ul> <li>\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 install_env.bat \u7684 <code>USE_MIRROR</code> \u9879\u6765\u51b3\u5b9a\u662f\u5426\u4f7f\u7528\u955c\u50cf\u7ad9\u4e0b\u8f7d\u3002</li> <li><code>USE_MIRROR=false</code> \u4f7f\u7528\u539f\u59cb\u7ad9\u4e0b\u8f7d\u6700\u65b0\u7a33\u5b9a\u7248 <code>torch</code> \u73af\u5883\u3002<code>USE_MIRROR=true</code> \u4e3a\u4ece\u955c\u50cf\u7ad9\u4e0b\u8f7d\u6700\u65b0 <code>torch</code> \u73af\u5883\u3002\u9ed8\u8ba4\u4e3a <code>true</code>\u3002</li> <li>\u53ef\u4ee5\u901a\u8fc7\u7f16\u8f91 install_env.bat \u7684 <code>INSTALL_TYPE</code> \u9879\u6765\u51b3\u5b9a\u662f\u5426\u542f\u7528\u53ef\u7f16\u8bd1\u73af\u5883\u4e0b\u8f7d\u3002</li> <li><code>INSTALL_TYPE=preview</code> \u4e0b\u8f7d\u5f00\u53d1\u7248\u7f16\u8bd1\u73af\u5883\u3002<code>INSTALL_TYPE=stable</code> \u4e0b\u8f7d\u7a33\u5b9a\u7248\u4e0d\u5e26\u7f16\u8bd1\u73af\u5883\u3002</li> </ul> </li> <li>\u82e5\u7b2c2\u6b65 INSTALL_TYPE=preview \u5219\u6267\u884c\u8fd9\u4e00\u6b65\uff08\u53ef\u8df3\u8fc7\uff0c\u6b64\u6b65\u4e3a\u6fc0\u6d3b\u7f16\u8bd1\u6a21\u578b\u73af\u5883\uff09       <ol> <li>\u4f7f\u7528\u5982\u4e0b\u94fe\u63a5\u4e0b\u8f7d LLVM \u7f16\u8bd1\u5668\u3002                <ul> <li>LLVM-17.0.6\uff08\u539f\u7ad9\u7ad9\u70b9\u4e0b\u8f7d\uff09</li> <li>LLVM-17.0.6\uff08\u955c\u50cf\u7ad9\u70b9\u4e0b\u8f7d\uff09</li> <li>\u4e0b\u8f7d\u5b8c LLVM-17.0.6-win64.exe \u540e\uff0c\u53cc\u51fb\u8fdb\u884c\u5b89\u88c5\uff0c\u9009\u62e9\u5408\u9002\u7684\u5b89\u88c5\u4f4d\u7f6e\uff0c\u6700\u91cd\u8981\u7684\u662f\u52fe\u9009 <code>Add Path to Current User</code> \u6dfb\u52a0\u73af\u5883\u53d8\u91cf\u3002</li> <li>\u786e\u8ba4\u5b89\u88c5\u5b8c\u6210\u3002</li> </ul> </li> <li>\u4e0b\u8f7d\u5b89\u88c5 Microsoft Visual C++ \u53ef\u518d\u53d1\u884c\u7a0b\u5e8f\u5305\uff0c\u89e3\u51b3\u6f5c\u5728 .dll \u4e22\u5931\u95ee\u9898\u3002                <ul> <li>MSVC++ 14.40.33810.0 \u4e0b\u8f7d</li> </ul> </li> <li>\u4e0b\u8f7d\u5b89\u88c5 Visual Studio \u793e\u533a\u7248\u4ee5\u83b7\u53d6 MSVC++ \u7f16\u8bd1\u5de5\u5177, \u89e3\u51b3 LLVM \u7684\u5934\u6587\u4ef6\u4f9d\u8d56\u95ee\u9898\u3002                <ul> <li>Visual Studio \u4e0b\u8f7d</li> <li>\u5b89\u88c5\u597dVisual Studio Installer\u4e4b\u540e\uff0c\u4e0b\u8f7dVisual Studio Community 2022</li> <li>\u5982\u4e0b\u56fe\u70b9\u51fb<code>\u4fee\u6539</code>\u6309\u94ae\uff0c\u627e\u5230<code>\u4f7f\u7528C++\u7684\u684c\u9762\u5f00\u53d1</code>\u9879\uff0c\u52fe\u9009\u4e0b\u8f7d</li> <p> </p> </ul> </li> </ol> </li> <li>\u53cc\u51fb start.bat, \u8fdb\u5165 Fish-Speech \u8bad\u7ec3\u63a8\u7406\u914d\u7f6e WebUI \u9875\u9762\u3002       <ul> <li>(\u53ef\u9009) \u60f3\u76f4\u63a5\u8fdb\u5165\u63a8\u7406\u9875\u9762\uff1f\u7f16\u8f91\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u7684 <code>API_FLAGS.txt</code>, \u524d\u4e09\u884c\u4fee\u6539\u6210\u5982\u4e0b\u683c\u5f0f:                <pre><code>--infer\n# --api\n# --listen ...\n...</code></pre> </li> <li>(\u53ef\u9009) \u60f3\u542f\u52a8 API \u670d\u52a1\u5668\uff1f\u7f16\u8f91\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u7684 <code>API_FLAGS.txt</code>, \u524d\u4e09\u884c\u4fee\u6539\u6210\u5982\u4e0b\u683c\u5f0f:                <pre><code># --infer\n--api\n--listen ...\n...</code></pre> </li> </ul> </li> <li>\uff08\u53ef\u9009\uff09\u53cc\u51fb <code>run_cmd.bat</code> \u8fdb\u5165\u672c\u9879\u76ee\u7684 conda/python \u547d\u4ee4\u884c\u73af\u5883</li> </ol>"},{"location":"#linux","title":"Linux \u914d\u7f6e","text":"<pre><code># \u521b\u5efa\u4e00\u4e2a python 3.10 \u865a\u62df\u73af\u5883, \u4f60\u4e5f\u53ef\u4ee5\u7528 virtualenv\nconda create -n fish-speech python=3.10\nconda activate fish-speech\n\n# \u5b89\u88c5 pytorch\npip3 install torch torchvision torchaudio\n\n# \u5b89\u88c5 fish-speech\npip3 install -e .\n\n# (Ubuntu / Debian \u7528\u6237) \u5b89\u88c5 sox\napt install libsox-dev\n</code></pre>"},{"location":"#_3","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<ul> <li>2024/05/10: \u66f4\u65b0\u4e86 Fish-Speech \u5230 1.1 \u7248\u672c\uff0c\u5f15\u5165\u4e86 VITS Decoder \u6765\u964d\u4f4e\u53e3\u80e1\u548c\u63d0\u9ad8\u97f3\u8272\u76f8\u4f3c\u5ea6.</li> <li>2024/04/22: \u5b8c\u6210\u4e86 Fish-Speech 1.0 \u7248\u672c, \u5927\u5e45\u4fee\u6539\u4e86 VQGAN \u548c LLAMA \u6a21\u578b.</li> <li>2023/12/28: \u6dfb\u52a0\u4e86 <code>lora</code> \u5fae\u8c03\u652f\u6301.</li> <li>2023/12/27: \u6dfb\u52a0\u4e86 <code>gradient checkpointing</code>, <code>causual sampling</code> \u548c <code>flash-attn</code> \u652f\u6301.</li> <li>2023/12/19: \u66f4\u65b0\u4e86 Webui \u548c HTTP API.</li> <li>2023/12/18: \u66f4\u65b0\u4e86\u5fae\u8c03\u6587\u6863\u548c\u76f8\u5173\u4f8b\u5b50.</li> <li>2023/12/17: \u66f4\u65b0\u4e86 <code>text2semantic</code> \u6a21\u578b, \u652f\u6301\u65e0\u97f3\u7d20\u6a21\u5f0f.</li> <li>2023/12/13: \u6d4b\u8bd5\u7248\u53d1\u5e03, \u5305\u542b VQGAN \u6a21\u578b\u548c\u4e00\u4e2a\u57fa\u4e8e LLAMA \u7684\u8bed\u8a00\u6a21\u578b (\u53ea\u652f\u6301\u97f3\u7d20).</li> </ul>"},{"location":"#_4","title":"\u81f4\u8c22","text":"<ul> <li>VITS2 (daniilrobnikov)</li> <li>Bert-VITS2</li> <li>GPT VITS</li> <li>MQTTS</li> <li>GPT Fast</li> <li>Transformers</li> <li>GPT-SoVITS</li> </ul>"},{"location":"finetune/","title":"\u5fae\u8c03","text":"<p>\u663e\u7136, \u5f53\u4f60\u6253\u5f00\u8fd9\u4e2a\u9875\u9762\u7684\u65f6\u5019, \u4f60\u5df2\u7ecf\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b few-shot \u7684\u6548\u679c\u4e0d\u7b97\u6ee1\u610f. \u4f60\u60f3\u8981\u5fae\u8c03\u4e00\u4e2a\u6a21\u578b, \u4f7f\u5f97\u5b83\u5728\u4f60\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d.  </p> <p><code>Fish Speech</code> \u7531\u4e09\u4e2a\u6a21\u5757\u7ec4\u6210: <code>VQGAN</code>,<code>LLAMA</code>, \u4ee5\u53ca <code>VITS Decoder</code>. </p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u5148\u8fdb\u884c\u5982\u4e0b\u6d4b\u8bd5\u6765\u5224\u65ad\u4f60\u662f\u5426\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code> <pre><code>python tools/vqgan/inference.py -i test.wav\npython tools/vits_decoder/inference.py \\\n    -ckpt checkpoints/vits_decoder_v1.1.ckpt \\\n    -i fake.npy -r test.wav \\\n    --text \"\u5408\u6210\u6587\u672c\"\n</code></pre> \u8be5\u6d4b\u8bd5\u4f1a\u751f\u6210\u4e00\u4e2a <code>fake.wav</code> \u6587\u4ef6, \u5982\u679c\u8be5\u6587\u4ef6\u7684\u97f3\u8272\u548c\u8bf4\u8bdd\u4eba\u7684\u97f3\u8272\u4e0d\u540c, \u6216\u8005\u8d28\u91cf\u4e0d\u9ad8, \u4f60\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code>.</p> <p>\u76f8\u5e94\u7684, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u6765\u8fd0\u884c <code>generate.py</code>, \u5224\u65ad\u97f5\u5f8b\u662f\u5426\u6ee1\u610f, \u5982\u679c\u4e0d\u6ee1\u610f, \u5219\u9700\u8981\u5fae\u8c03 <code>LLAMA</code>.</p> <p>\u5efa\u8bae\u5148\u5bf9 <code>LLAMA</code> \u8fdb\u884c\u5fae\u8c03\uff0c\u6700\u540e\u518d\u6839\u636e\u9700\u8981\u5fae\u8c03 <code>VITS Decoder</code>.</p>"},{"location":"finetune/#llama","title":"LLAMA \u5fae\u8c03","text":""},{"location":"finetune/#1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u5efa\u8bae\u4e3a <code>.lab</code>.</p> <p>Warning</p> <p>\u5efa\u8bae\u5148\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u54cd\u5ea6\u5339\u914d, \u4f60\u53ef\u4ee5\u4f7f\u7528 fish-audio-preprocess \u6765\u5b8c\u6210\u8fd9\u4e00\u6b65\u9aa4.  <pre><code>fap loudness-norm data-raw data --clean\n</code></pre></p>"},{"location":"finetune/#2-token","title":"2. \u6279\u91cf\u63d0\u53d6\u8bed\u4e49 token","text":"<p>\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 vqgan \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\n</code></pre> <p>\u968f\u540e\u53ef\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u63d0\u53d6\u8bed\u4e49 token:</p> <pre><code>python tools/vqgan/extract_vq.py data \\\n    --num-workers 1 --batch-size 16 \\\n    --config-name \"vqgan_pretrain\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u8c03\u6574 <code>--num-workers</code> \u548c <code>--batch-size</code> \u6765\u63d0\u9ad8\u63d0\u53d6\u901f\u5ea6, \u4f46\u662f\u8bf7\u6ce8\u610f\u4e0d\u8981\u8d85\u8fc7\u4f60\u7684\u663e\u5b58\u9650\u5236.  </p> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>.npy</code> \u6587\u4ef6, \u5982\u4e0b\u6240\u793a:</p> <pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 21.15-26.44.npy\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.npy\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u251c\u2500\u2500 30.1-32.71.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.npy\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u251c\u2500\u2500 38.79-40.85.mp3\n    \u2514\u2500\u2500 38.79-40.85.npy\n</code></pre>"},{"location":"finetune/#3-protobuf","title":"3. \u6253\u5305\u6570\u636e\u96c6\u4e3a protobuf","text":"<pre><code>python tools/llama/build_dataset.py \\\n    --input \"data\" \\\n    --output \"data/quantized-dataset-ft.protos\" \\\n    --text-extension .lab \\\n    --num-workers 16\n</code></pre> <p>\u547d\u4ee4\u6267\u884c\u5b8c\u6bd5\u540e, \u4f60\u5e94\u8be5\u80fd\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u770b\u5230 <code>quantized-dataset-ft.protos</code> \u6587\u4ef6.</p>"},{"location":"finetune/#4","title":"4. \u6700\u540e, \u542f\u52a8\u5fae\u8c03","text":"<p>\u540c\u6837\u7684, \u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\u4e0b\u8f7d\u4e86 <code>LLAMA</code> \u6743\u91cd, \u5982\u679c\u6ca1\u6709, \u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4:</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237, \u53ef\u4f7f\u7528 mirror \u4e0b\u8f7d.</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\n</code></pre> <p>\u6700\u540e, \u4f60\u53ef\u4ee5\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8\u5fae\u8c03:</p> <pre><code>python fish_speech/train.py --config-name text2semantic_finetune \\\n    model@model.model=dual_ar_2_codebook_medium\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/text2semantic_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570\u5982 <code>batch_size</code>, <code>gradient_accumulation_steps</code> \u7b49, \u6765\u9002\u5e94\u4f60\u7684\u663e\u5b58.</p> <p>Note</p> <p>\u5bf9\u4e8e Windows \u7528\u6237, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>trainer.strategy.process_group_backend=gloo</code> \u6765\u907f\u514d <code>nccl</code> \u7684\u95ee\u9898.</p> <p>\u8bad\u7ec3\u7ed3\u675f\u540e, \u4f60\u53ef\u4ee5\u53c2\u8003 \u63a8\u7406 \u90e8\u5206, \u5e76\u643a\u5e26 <code>--speaker SPK1</code> \u53c2\u6570\u6765\u6d4b\u8bd5\u4f60\u7684\u6a21\u578b.</p> <p>Info</p> <p>\u9ed8\u8ba4\u914d\u7f6e\u4e0b, \u57fa\u672c\u53ea\u4f1a\u5b66\u5230\u8bf4\u8bdd\u4eba\u7684\u53d1\u97f3\u65b9\u5f0f, \u800c\u4e0d\u5305\u542b\u97f3\u8272, \u4f60\u4f9d\u7136\u9700\u8981\u4f7f\u7528 prompt \u6765\u4fdd\u8bc1\u97f3\u8272\u7684\u7a33\u5b9a\u6027. \u5982\u679c\u4f60\u60f3\u8981\u5b66\u5230\u97f3\u8272, \u8bf7\u5c06\u8bad\u7ec3\u6b65\u6570\u8c03\u5927, \u4f46\u8fd9\u6709\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408.</p>"},{"location":"finetune/#lora","title":"\u4f7f\u7528 LoRA \u8fdb\u884c\u5fae\u8c03","text":"<p>Note</p> <p>LoRA \u53ef\u4ee5\u51cf\u5c11\u6a21\u578b\u8fc7\u62df\u5408\u7684\u98ce\u9669, \u4f46\u662f\u76f8\u5e94\u7684\u4f1a\u5bfc\u81f4\u5728\u5927\u6570\u636e\u96c6\u4e0a\u6b20\u62df\u5408.   </p> <p>\u5982\u679c\u4f60\u60f3\u4f7f\u7528 LoRA, \u8bf7\u6dfb\u52a0\u4ee5\u4e0b\u53c2\u6570 <code>+lora@model.lora_config=r_8_alpha_16</code>.  </p> <p>\u8bad\u7ec3\u5b8c\u6210\u540e, \u4f60\u9700\u8981\u5148\u5c06 loRA \u7684\u6743\u91cd\u8f6c\u4e3a\u666e\u901a\u6743\u91cd, \u7136\u540e\u518d\u8fdb\u884c\u63a8\u7406.</p> <pre><code>python tools/llama/merge_lora.py \\\n    --llama-config dual_ar_2_codebook_medium \\\n    --lora-config r_8_alpha_16 \\\n    --llama-weight checkpoints/text2semantic-sft-medium-v1.1-4k.pth \\\n    --lora-weight results/text2semantic-finetune-medium-lora/checkpoints/step_000000200.ckpt \\\n    --output checkpoints/merged.ckpt\n</code></pre>"},{"location":"finetune/#vits","title":"VITS \u5fae\u8c03","text":""},{"location":"finetune/#1_1","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.lab\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.lab\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u251c\u2500\u2500 30.1-32.71.lab\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u251c\u2500\u2500 38.79-40.85.lab\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>Note</p> <p>VITS \u5fae\u8c03\u76ee\u524d\u4ec5\u652f\u6301 <code>.lab</code> \u4f5c\u4e3a\u6807\u7b7e\u6587\u4ef6\uff0c\u4e0d\u652f\u6301 <code>filelist</code> \u5f62\u5f0f.</p> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>, \u6807\u6ce8\u6587\u4ef6\u540e\u7f00\u5efa\u8bae\u4e3a <code>.lab</code>.</p>"},{"location":"finetune/#2","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/vq_train_filelist.txt</code> \u548c <code>data/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vits_decoder_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vits_decoder_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u5982\u6570\u636e\u96c6\u914d\u7f6e.</p>"},{"location":"finetune/#4_1","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path results/vits_decoder_finetune/checkpoints/step_000010000.ckpt \\\n    -i test.npy -r test.wav \\\n    --text \"\u5408\u6210\u6587\u672c\"\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p>"},{"location":"finetune/#vqgan","title":"VQGAN \u5fae\u8c03 (\u4e0d\u63a8\u8350)","text":"<p>\u5728 V1.1 \u7248\u672c\u4e2d, \u6211\u4eec\u4e0d\u518d\u63a8\u8350\u4f7f\u7528 VQGAN \u8fdb\u884c\u5fae\u8c03, \u4f7f\u7528 VITS Decoder \u4f1a\u83b7\u5f97\u66f4\u597d\u7684\u8868\u73b0, \u4f46\u662f\u5982\u679c\u4f60\u4ecd\u7136\u60f3\u8981\u4f7f\u7528 VQGAN \u8fdb\u884c\u5fae\u8c03, \u4f60\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u6b65\u9aa4.</p>"},{"location":"finetune/#1_2","title":"1. \u51c6\u5907\u6570\u636e\u96c6","text":"<pre><code>.\n\u251c\u2500\u2500 SPK1\n\u2502   \u251c\u2500\u2500 21.15-26.44.mp3\n\u2502   \u251c\u2500\u2500 27.51-29.98.mp3\n\u2502   \u2514\u2500\u2500 30.1-32.71.mp3\n\u2514\u2500\u2500 SPK2\n    \u2514\u2500\u2500 38.79-40.85.mp3\n</code></pre> <p>\u4f60\u9700\u8981\u5c06\u6570\u636e\u96c6\u8f6c\u4e3a\u4ee5\u4e0a\u683c\u5f0f, \u5e76\u653e\u5230 <code>data</code> \u4e0b, \u97f3\u9891\u540e\u7f00\u53ef\u4ee5\u4e3a <code>.mp3</code>, <code>.wav</code> \u6216 <code>.flac</code>.</p>"},{"location":"finetune/#2_1","title":"2. \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6","text":"<pre><code>python tools/vqgan/create_train_split.py data\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728 <code>data</code> \u76ee\u5f55\u4e0b\u521b\u5efa <code>data/vq_train_filelist.txt</code> \u548c <code>data/vq_val_filelist.txt</code> \u6587\u4ef6, \u5206\u522b\u7528\u4e8e\u8bad\u7ec3\u548c\u9a8c\u8bc1.  </p> <p>Info</p> <p>\u5bf9\u4e8e VITS \u683c\u5f0f, \u4f60\u53ef\u4ee5\u4f7f\u7528 <code>--filelist xxx.list</code> \u6765\u6307\u5b9a\u6587\u4ef6\u5217\u8868. \u8bf7\u6ce8\u610f, <code>filelist</code> \u6240\u6307\u5411\u7684\u97f3\u9891\u6587\u4ef6\u5fc5\u987b\u4e5f\u4f4d\u4e8e <code>data</code> \u6587\u4ef6\u5939\u4e0b.</p>"},{"location":"finetune/#3_1","title":"3. \u542f\u52a8\u8bad\u7ec3","text":"<pre><code>python fish_speech/train.py --config-name vqgan_finetune\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539 <code>fish_speech/configs/vqgan_finetune.yaml</code> \u6765\u4fee\u6539\u8bad\u7ec3\u53c2\u6570, \u4f46\u5927\u90e8\u5206\u60c5\u51b5\u4e0b, \u4f60\u4e0d\u9700\u8981\u8fd9\u4e48\u505a.</p>"},{"location":"finetune/#4_2","title":"4. \u6d4b\u8bd5\u97f3\u9891","text":"<pre><code>python tools/vqgan/inference.py -i test.wav --checkpoint-path results/vqgan_finetune/checkpoints/step_000010000.ckpt\n</code></pre> <p>\u4f60\u53ef\u4ee5\u67e5\u770b <code>fake.wav</code> \u6765\u5224\u65ad\u5fae\u8c03\u6548\u679c.</p> <p>Note</p> <p>\u4f60\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u5176\u4ed6\u7684 checkpoint, \u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528\u6700\u65e9\u7684\u6ee1\u8db3\u4f60\u8981\u6c42\u7684 checkpoint, \u4ed6\u4eec\u901a\u5e38\u5728 OOD \u4e0a\u8868\u73b0\u66f4\u597d.</p>"},{"location":"inference/","title":"\u63a8\u7406","text":"<p>\u63a8\u7406\u652f\u6301\u547d\u4ee4\u884c, http api, \u4ee5\u53ca webui \u4e09\u79cd\u65b9\u5f0f.  </p> <p>Note</p> <p>\u603b\u7684\u6765\u8bf4, \u63a8\u7406\u5206\u4e3a\u51e0\u4e2a\u90e8\u5206:  </p> <ol> <li>\u7ed9\u5b9a\u4e00\u6bb5 ~10 \u79d2\u7684\u8bed\u97f3, \u5c06\u5b83\u7528 VQGAN \u7f16\u7801.  </li> <li>\u5c06\u7f16\u7801\u540e\u7684\u8bed\u4e49 token \u548c\u5bf9\u5e94\u6587\u672c\u8f93\u5165\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4f8b\u5b50.  </li> <li>\u7ed9\u5b9a\u4e00\u6bb5\u65b0\u6587\u672c, \u8ba9\u6a21\u578b\u751f\u6210\u5bf9\u5e94\u7684\u8bed\u4e49 token.  </li> <li>\u5c06\u751f\u6210\u7684\u8bed\u4e49 token \u8f93\u5165 VITS / VQGAN \u89e3\u7801, \u751f\u6210\u5bf9\u5e94\u7684\u8bed\u97f3.  </li> </ol> <p>\u5728 V1.1 \u7248\u672c\u4e2d, \u6211\u4eec\u63a8\u8350\u4f18\u5148\u4f7f\u7528 VITS \u89e3\u7801\u5668, \u56e0\u4e3a\u5b83\u5728\u97f3\u8d28\u548c\u53e3\u80e1\u4e0a\u90fd\u6709\u66f4\u597d\u7684\u8868\u73b0.</p>"},{"location":"inference/#_2","title":"\u547d\u4ee4\u884c\u63a8\u7406","text":"<p>\u4ece\u6211\u4eec\u7684 huggingface \u4ed3\u5e93\u4e0b\u8f7d\u6240\u9700\u7684 <code>vqgan</code> \u548c <code>text2semantic</code> \u6a21\u578b\u3002</p> <pre><code>huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 vits_decoder_v1.1.ckpt --local-dir checkpoints\nhuggingface-cli download fishaudio/fish-speech-1 firefly-gan-base-generator.ckpt --local-dir checkpoints\n</code></pre> <p>\u5bf9\u4e8e\u4e2d\u56fd\u5927\u9646\u7528\u6237\uff0c\u53ef\u4f7f\u7528mirror\u4e0b\u8f7d\u3002</p> <pre><code>HF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vq-gan-group-fsq-2x1024.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 text2semantic-sft-medium-v1.1-4k.pth --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 vits_decoder_v1.1.ckpt --local-dir checkpoints\nHF_ENDPOINT=https://hf-mirror.com huggingface-cli download fishaudio/fish-speech-1 firefly-gan-base-generator.ckpt --local-dir checkpoints\n</code></pre>"},{"location":"inference/#1-prompt","title":"1. \u4ece\u8bed\u97f3\u751f\u6210 prompt:","text":"<p>Note</p> <p>\u5982\u679c\u4f60\u6253\u7b97\u8ba9\u6a21\u578b\u968f\u673a\u9009\u62e9\u97f3\u8272, \u4f60\u53ef\u4ee5\u8df3\u8fc7\u8fd9\u4e00\u6b65.</p> <p><pre><code>python tools/vqgan/inference.py \\\n    -i \"paimon.wav\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre> \u4f60\u5e94\u8be5\u80fd\u5f97\u5230\u4e00\u4e2a <code>fake.npy</code> \u6587\u4ef6.</p>"},{"location":"inference/#2-token","title":"2. \u4ece\u6587\u672c\u751f\u6210\u8bed\u4e49 token:","text":"<pre><code>python tools/llama/generate.py \\\n    --text \"\u8981\u8f6c\u6362\u7684\u6587\u672c\" \\\n    --prompt-text \"\u4f60\u7684\u53c2\u8003\u6587\u672c\" \\\n    --prompt-tokens \"fake.npy\" \\\n    --config-name dual_ar_2_codebook_medium \\\n    --checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --num-samples 2 \\\n    --compile\n</code></pre> <p>\u8be5\u547d\u4ee4\u4f1a\u5728\u5de5\u4f5c\u76ee\u5f55\u4e0b\u521b\u5efa <code>codes_N</code> \u6587\u4ef6, \u5176\u4e2d N \u662f\u4ece 0 \u5f00\u59cb\u7684\u6574\u6570.</p> <p>Note</p> <p>\u60a8\u53ef\u80fd\u5e0c\u671b\u4f7f\u7528 <code>--compile</code> \u6765\u878d\u5408 cuda \u5185\u6838\u4ee5\u5b9e\u73b0\u66f4\u5feb\u7684\u63a8\u7406 (~30 \u4e2a token/\u79d2 -&gt; ~500 \u4e2a token/\u79d2). \u5bf9\u5e94\u7684, \u5982\u679c\u4f60\u4e0d\u6253\u7b97\u4f7f\u7528\u52a0\u901f, \u4f60\u53ef\u4ee5\u6ce8\u91ca\u6389 <code>--compile</code> \u53c2\u6570.</p> <p>Info</p> <p>\u5bf9\u4e8e\u4e0d\u652f\u6301 bf16 \u7684 GPU, \u4f60\u53ef\u80fd\u9700\u8981\u4f7f\u7528 <code>--half</code> \u53c2\u6570.</p> <p>Warning</p> <p>\u5982\u679c\u4f60\u5728\u4f7f\u7528\u81ea\u5df1\u5fae\u8c03\u7684\u6a21\u578b, \u8bf7\u52a1\u5fc5\u643a\u5e26 <code>--speaker</code> \u53c2\u6570\u6765\u4fdd\u8bc1\u53d1\u97f3\u7684\u7a33\u5b9a\u6027.</p>"},{"location":"inference/#3-token","title":"3. \u4ece\u8bed\u4e49 token \u751f\u6210\u4eba\u58f0:","text":""},{"location":"inference/#vits","title":"VITS \u89e3\u7801","text":"<pre><code>python tools/vits_decoder/inference.py \\\n    --checkpoint-path checkpoints/vits_decoder_v1.1.ckpt \\\n    -i codes_0.npy -r ref.wav \\\n    --text \"\u8981\u751f\u6210\u7684\u6587\u672c\"\n</code></pre>"},{"location":"inference/#vqgan","title":"VQGAN \u89e3\u7801 (\u4e0d\u63a8\u8350)","text":"<pre><code>python tools/vqgan/inference.py \\\n    -i \"codes_0.npy\" \\\n    --checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\"\n</code></pre>"},{"location":"inference/#http-api","title":"HTTP API \u63a8\u7406","text":"<p>\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:</p> <pre><code>python -m tools.api \\\n    --listen 0.0.0.0:8000 \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --decoder-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --decoder-config-name vqgan_pretrain\n\n# \u63a8\u8350\u4e2d\u56fd\u5927\u9646\u7528\u6237\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 HTTP \u670d\u52a1:\nHF_ENDPOINT=https://hf-mirror.com python -m ...\n</code></pre> <p>\u968f\u540e, \u4f60\u53ef\u4ee5\u5728 <code>http://127.0.0.1:8000/</code> \u4e2d\u67e5\u770b\u5e76\u6d4b\u8bd5 API.</p> <p>Info</p> <p>\u4f60\u5e94\u8be5\u4f7f\u7528\u4ee5\u4e0b\u53c2\u6570\u6765\u542f\u52a8 VITS \u89e3\u7801\u5668:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # \u6216\u8005\u4f60\u81ea\u5df1\u7684\u6a21\u578b\n</code></pre>"},{"location":"inference/#webui","title":"WebUI \u63a8\u7406","text":"<p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u6765\u542f\u52a8 WebUI:</p> <pre><code>python -m tools.webui \\\n    --llama-checkpoint-path \"checkpoints/text2semantic-sft-medium-v1.1-4k.pth\" \\\n    --llama-config-name dual_ar_2_codebook_medium \\\n    --decoder-checkpoint-path \"checkpoints/vq-gan-group-fsq-2x1024.pth\" \\\n    --decoder-config-name vqgan_pretrain\n</code></pre> <p>Info</p> <p>\u4f60\u5e94\u8be5\u4f7f\u7528\u4ee5\u4e0b\u53c2\u6570\u6765\u542f\u52a8 VITS \u89e3\u7801\u5668:</p> <pre><code>--decoder-config-name vits_decoder_finetune \\\n--decoder-checkpoint-path \"checkpoints/vits_decoder_v1.1.ckpt\" # \u6216\u8005\u4f60\u81ea\u5df1\u7684\u6a21\u578b\n</code></pre> <p>Note</p> <p>\u4f60\u53ef\u4ee5\u4f7f\u7528 Gradio \u73af\u5883\u53d8\u91cf, \u5982 <code>GRADIO_SHARE</code>, <code>GRADIO_SERVER_PORT</code>, <code>GRADIO_SERVER_NAME</code> \u6765\u914d\u7f6e WebUI.</p> <p>\u795d\u5927\u5bb6\u73a9\u5f97\u5f00\u5fc3!</p>"},{"location":"samples/","title":"\u4f8b\u5b50","text":""},{"location":"samples/#1","title":"\u4e2d\u6587\u53e5\u5b50 1","text":"<pre><code>\u4eba\u95f4\u706f\u706b\u5012\u6620\u6e56\u4e2d\uff0c\u5979\u7684\u6e34\u671b\u8ba9\u9759\u6c34\u6cdb\u8d77\u6d9f\u6f2a\u3002\u82e5\u4ee3\u4ef7\u53ea\u662f\u5b64\u72ec\uff0c\u90a3\u5c31\u8ba9\u8fd9\u4efd\u613f\u671b\u8086\u610f\u6d41\u6dcc\u3002\n\u6d41\u5165\u5979\u6240\u6ce8\u89c6\u7684\u4e16\u95f4\uff0c\u4e5f\u6d41\u5165\u5979\u5982\u6e56\u6c34\u822c\u6f84\u6f88\u7684\u76ee\u5149\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u949f\u79bb (\u539f\u795e) \u8299\u5b81\u5a1c (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2","title":"\u4e2d\u6587\u53e5\u5b50 2","text":"<pre><code>\u4f60\u4eec\u8fd9\u4e2a\u662f\u4ec0\u4e48\u7fa4\u554a\uff0c\u4f60\u4eec\u8fd9\u662f\u5bb3\u4eba\u4e0d\u6d45\u554a\u4f60\u4eec\u8fd9\u4e2a\u7fa4\uff01\u8c01\u662f\u7fa4\u4e3b\uff0c\u51fa\u6765\uff01\u771f\u7684\u592a\u8fc7\u5206\u4e86\u3002\u4f60\u4eec\u641e\u8fd9\u4e2a\u7fa4\u5e72\u4ec0\u4e48\uff1f\n\u6211\u513f\u5b50\u6bcf\u4e00\u79d1\u7684\u6210\u7ee9\u90fd\u4e0d\u8fc7\u90a3\u4e2a\u5e73\u5747\u5206\u5450\uff0c\u4ed6\u73b0\u5728\u521d\u4e8c\uff0c\u4f60\u53eb\u6211\u513f\u5b50\u600e\u4e48\u529e\u554a\uff1f\u4ed6\u73b0\u5728\u8fd8\u4e0d\u5230\u9ad8\u4e2d\u554a\uff1f\n\u4f60\u4eec\u5bb3\u6b7b\u6211\u513f\u5b50\u4e86\uff01\u5feb\u70b9\u51fa\u6765\u4f60\u8fd9\u4e2a\u7fa4\u4e3b\uff01\u518d\u8fd9\u6837\u6211\u53bb\u62a5\u8b66\u4e86\u554a\uff01\u6211\u8ddf\u4f60\u4eec\u8bf4\u4f60\u4eec\u8fd9\u4e00\u5e2e\u4eba\u554a\uff0c\u4e00\u5929\u5230\u665a\u554a\uff0c\n\u641e\u8fd9\u4e9b\u4ec0\u4e48\u6e38\u620f\u554a\uff0c\u52a8\u6f2b\u554a\uff0c\u4f1a\u5bb3\u6b7b\u4f60\u4eec\u7684\uff0c\u4f60\u4eec\u6ca1\u6709\u524d\u9014\u6211\u8ddf\u4f60\u8bf4\u3002\u4f60\u4eec\u8fd9\u4e5d\u767e\u591a\u4e2a\u4eba\uff0c\u597d\u597d\u5b66\u4e60\u4e0d\u597d\u5417\uff1f\n\u4e00\u5929\u5230\u665a\u5728\u4e0a\u7f51\u3002\u6709\u4ec0\u4e48\u610f\u601d\u554a\uff1f\u9ebb\u70e6\u4f60\u91cd\u89c6\u4e00\u4e0b\u4f60\u4eec\u7684\u751f\u6d3b\u7684\u76ee\u6807\u554a\uff1f\u6709\u4e00\u70b9\u5b66\u4e60\u76ee\u6807\u884c\u4e0d\u884c\uff1f\u4e00\u5929\u5230\u665a\u4e0a\u7f51\u662f\u4e0d\u662f\u4eba\u554a\uff1f\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u7eb3\u897f\u59b2 (\u539f\u795e) \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#3","title":"\u4e2d\u6587\u53e5\u5b50 3","text":"<pre><code>\u5927\u5bb6\u597d\uff0c\u6211\u662f Fish Audio \u5f00\u53d1\u7684\u5f00\u6e90\u6587\u672c\u8f6c\u8bed\u97f3\u6a21\u578b\u3002\u7ecf\u8fc7\u5341\u4e94\u4e07\u5c0f\u65f6\u7684\u6570\u636e\u8bad\u7ec3\uff0c\n\u6211\u5df2\u7ecf\u80fd\u591f\u719f\u7ec3\u638c\u63e1\u4e2d\u6587\u3001\u65e5\u8bed\u548c\u82f1\u8bed\uff0c\u6211\u7684\u8bed\u8a00\u5904\u7406\u80fd\u529b\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\uff0c\u58f0\u97f3\u8868\u73b0\u5f62\u5f0f\u4e30\u5bcc\u591a\u53d8\u3002\n\u4f5c\u4e3a\u4e00\u4e2a\u4ec5\u6709\u4ebf\u7ea7\u53c2\u6570\u7684\u6a21\u578b\uff0c\u6211\u76f8\u4fe1\u793e\u533a\u6210\u5458\u80fd\u591f\u5728\u4e2a\u4eba\u8bbe\u5907\u4e0a\u8f7b\u677e\u8fd0\u884c\u548c\u5fae\u8c03\uff0c\u8ba9\u6211\u6210\u4e3a\u60a8\u7684\u79c1\u4eba\u8bed\u97f3\u52a9\u624b\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#1_1","title":"\u82f1\u6587\u53e5\u5b50 1","text":"<pre><code>In the realm of advanced technology, the evolution of artificial intelligence stands as a \nmonumental achievement. This dynamic field, constantly pushing the boundaries of what \nmachines can do, has seen rapid growth and innovation. From deciphering complex data \npatterns to driving cars autonomously, AI's applications are vast and diverse.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2_1","title":"\u82f1\u6587\u53e5\u5b50 2","text":"<pre><code>Hello everyone, I am an open-source text-to-speech model developed by \nFish Audio. After training with 150,000 hours of data, I have become proficient \nin Chinese, Japanese, and English, and my language processing abilities \nare close to human level. My voice is capable of a wide range of expressions. \nAs a model with only hundreds of millions of parameters, I believe community \nmembers can easily run and fine-tune me on their personal devices, allowing \nme to serve as your personal voice assistant.\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"},{"location":"samples/#1_2","title":"\u65e5\u6587\u53e5\u5b50 1","text":"<pre><code>\u5148\u9032\u6280\u8853\u306e\u9818\u57df\u306b\u304a\u3044\u3066\u3001\u4eba\u5de5\u77e5\u80fd\u306e\u9032\u5316\u306f\u753b\u671f\u7684\u306a\u6210\u679c\u3068\u3057\u3066\u7acb\u3063\u3066\u3044\u307e\u3059\u3002\u5e38\u306b\u6a5f\u68b0\u304c\u3067\u304d\u308b\u3053\u3068\u306e\u9650\u754c\u3092\n\u62bc\u3057\u5e83\u3052\u3066\u3044\u308b\u3053\u306e\u30c0\u30a4\u30ca\u30df\u30c3\u30af\u306a\u5206\u91ce\u306f\u3001\u6025\u901f\u306a\u6210\u9577\u3068\u9769\u65b0\u3092\u898b\u305b\u3066\u3044\u307e\u3059\u3002\u8907\u96d1\u306a\u30c7\u30fc\u30bf\u30d1\u30bf\u30fc\u30f3\u306e\u89e3\u8aad\u304b\n\u3089\u81ea\u52d5\u904b\u8ee2\u8eca\u306e\u64cd\u7e26\u307e\u3067\u3001AI\u306e\u5fdc\u7528\u306f\u5e83\u7bc4\u56f2\u306b\u53ca\u3073\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba 1  -  \u968f\u673a\u8bf4\u8bdd\u4eba 2  -"},{"location":"samples/#2_2","title":"\u65e5\u6587\u53e5\u5b50 2","text":"<pre><code>\u7686\u3055\u3093\u3001\u3053\u3093\u306b\u3061\u306f\u3002\u79c1\u306f\u30d5\u30a3\u30c3\u30b7\u30e5\u30aa\u30fc\u30c7\u30a3\u30aa\u306b\u3088\u3063\u3066\u958b\u767a\u3055\u308c\u305f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306e\u30c6\n\u30ad\u30b9\u30c8\u304b\u3089\u97f3\u58f0\u3078\u306e\u5909\u63db\u30e2\u30c7\u30eb\u3067\u3059\u300215\u4e07\u6642\u9593\u306e\u30c7\u30fc\u30bf\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3092\u7d4c\u3066\u3001\n\u4e2d\u56fd\u8a9e\u3001\u65e5\u672c\u8a9e\u3001\u82f1\u8a9e\u3092\u719f\u77e5\u3057\u3066\u304a\u308a\u3001\u8a00\u8a9e\u51e6\u7406\u80fd\u529b\u306f\u4eba\u9593\u306b\u8fd1\u3044\u30ec\u30d9\u30eb\u3067\u3059\u3002\n\u58f0\u306e\u8868\u73fe\u3082\u591a\u5f69\u3067\u8c4a\u304b\u3067\u3059\u3002\u6570\u5104\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6301\u3064\u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\n\u306e\u30e1\u30f3\u30d0\u30fc\u304c\u500b\u4eba\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u7c21\u5358\u306b\u5b9f\u884c\u3057\u3001\u5fae\u8abf\u6574\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3068\n\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u79c1\u3092\u500b\u4eba\u306e\u97f3\u58f0\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3068\u3057\u3066\u6d3b\u7528\u3067\u304d\u307e\u3059\u3002\n</code></pre> \u8bf4\u8bdd\u4eba \u8f93\u5165\u97f3\u9891 \u5408\u6210\u97f3\u9891 \u968f\u673a\u8bf4\u8bdd\u4eba  -"}]}